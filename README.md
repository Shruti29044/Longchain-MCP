# ğŸ¤– LangChain MCP Agent (with Ollama & Slack Integration)

A powerful AI-driven backend that uses **LangChain agents** and **custom tools** to summarize GitHub Pull Requests and notify your team on **Slack** â€” built with **FastAPI**, integrated with **Ollama (LLaMA 3)** and optionally **OpenAI**.

---

## ğŸ“¦ What Does This Project Do?

This project simulates an **MCP-style LangChain agent** that automates:

- âœ… **Summarizing PRs** in friendly, Slack-ready format
- âœ… **Suggesting PR labels** based on file types changed
- âœ… **Sending Slack notifications** using a webhook
- âœ… Running entirely **locally** using **Ollama**
- âœ… OR, optionally using **OpenAI API**

---

## ğŸ§  Architecture Overview

| Component          | Description                                                                 |
|-------------------|-----------------------------------------------------------------------------|
| ğŸ§© LangChain Agent | Uses a ReAct agent with multiple tools                                       |
| ğŸ› ï¸ Tools           | Functions for PR summarization, labeling, Slack alerts                       |
| ğŸ’¬ PromptTemplate  | A formatted instruction prompt for summarization                             |
| ğŸŒ Slack           | Sends updates using your Slack webhook                                       |
| ğŸ§  Ollama (LLaMA3) | The default local LLM provider (no internet or API key needed!)              |
| ğŸ§  OpenAI (optional) | You can switch to GPT-3.5/4 if needed via `.env`                             |

---

## ğŸš€ Features

- ğŸ” Fully local LLM support via **Ollama**
- ğŸŒ Optionally switch to **OpenAI** by flipping one line in `.env`
- ğŸ“© Instant **Slack alerts** after processing a PR
- âš™ï¸ Configurable and extendable tool system
- âš¡ FastAPI server with a clean `/agent` endpoint

---

## ğŸ“ Project Structure

```bash
complex_langchain_mcp/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py             # Main FastAPI + LangChain app
â”‚   â”œâ”€â”€ .env                # LLM provider + Slack webhook
â”‚   â”œâ”€â”€ requirements.txt    # Dependencies
â”‚   â””â”€â”€ venv/               # Virtual environment (not pushed to Git)
````

---

## ğŸ§ª How It Works

1. You send a POST request with a task:

   ```json
   {
     "task": "summarize this PR: fixed bug in api/utils.py"
   }
   ```

2. The **LangChain agent** processes the task:

   * Determines what tool to use
   * Uses the `SummarizePR` tool with a smart prompt
   * Then calls the `NotifySlack` tool to alert your team

3. A message like this appears on Slack:

   ```
   â€¢ ğŸ› Bug fix alert!
   â€¢ Fixed an issue in `api/utils.py` ğŸ‘€
   ```

---

## ğŸ§° Tech Stack

* **Python 3.11+**
* [FastAPI](https://fastapi.tiangolo.com/) â€” Backend framework
* [LangChain](https://www.langchain.com/) â€” Agent and tools
* [Ollama](https://ollama.com/) â€” Local LLM engine (used: `llama3`)
* [Slack Webhooks](https://api.slack.com/messaging/webhooks) â€” Messaging
* Optional: [OpenAI API](https://platform.openai.com/)

---

## âš™ï¸ Setup Instructions

### âœ… Prerequisites

* Python 3.11+
* [Ollama installed](https://ollama.com/download) with `llama3` pulled
* GitHub + Slack webhook URL

---

### ğŸ§ª 1. Clone the Repo

```bash
git clone https://github.com/Shruti29044/Longchain-MCP.git
cd Longchain-MCP/app
```

---

### ğŸ§ª 2. Create a Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate   # On Windows
```

---

### ğŸ§ª 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

### ğŸ§ª 4. Configure `.env`

```ini
LLM_PROVIDER=ollama
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/XXX/YYY/ZZZ
# Optional for OpenAI:
# OPENAI_API_KEY=sk-...
```

---

### ğŸ§ª 5. Start Ollama (if using it)

```bash
ollama run llama3
```

---

### ğŸ§ª 6. Run the Server

```bash
uvicorn main:app --reload --port 8080
```

---

### âœ… 7. Test the API

```bash
curl -X POST http://127.0.0.1:8080/agent ^
  -H "Content-Type: application/json" ^
  -d "{\"task\": \"summarize this PR: fixed bug in api/utils.py\"}"
```

---

## ğŸ”„ Switching from Ollama to OpenAI

Just edit your `.env`:

```ini
LLM_PROVIDER=openai
OPENAI_API_KEY=your-key-here
```

Thatâ€™s it! OpenAI will now be used automatically.

---

## ğŸš§ Challenges Faced

| Challenge                          | Solution                                                                |
| ---------------------------------- | ----------------------------------------------------------------------- |
| ğŸ” Slack messages weren't posting  | Set the correct webhook URL in `.env` and made sure Slack app is active |
| âŒ Curl not working on Windows CMD  | Used PowerShell or split into one-line `curl` instead                   |
| ğŸ’¥ FastAPI errors                  | Ensured `task` was sent in proper JSON via body                         |
| âš ï¸ LangChain deprecation warnings  | Noted migration to LangGraph (but this still works fine)                |
| ğŸ” Ollama model not found          | Had to run `ollama pull llama3` before `ollama run llama3`              |
| ğŸ§  Understanding LangChain tooling | Took time to understand `Tool`, `PromptTemplate`, `initialize_agent`    |

---

## ğŸ“Œ Example Slack Message

```
â€¢ ğŸ› Bug fix alert! ğŸ”§
â€¢ Fixed an issue in `api/utils.py` ğŸ‘€
```

---

## ğŸ™Œ Future Ideas

* GitHub API integration (auto-fetch PRs)
* Use LangGraph instead of ReAct agent
* Add support for Jira, Discord, etc.
* Automatic code quality summaries

---

## ğŸ“œ License

MIT â€” use it freely, improve it, and share with the community!

---

## ğŸ’¬ Made with â¤ï¸ by Shruti

